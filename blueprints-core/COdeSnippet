from langchain_community.document_loaders import PyPDFLoader
loader=PyPDFLoader("sc-2024.pdf")
docs=loader.load()
docs[0]


from langchain_text_splitters import RecursiveCharacterTextSplitter

text_splitter=RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50) #chunk_size and chunk_overlap can be modified accordingly
final_documents=text_splitter.split_documents(docs) #docs taken from parsed pdf
print(final_documents[0])

url = "http://192.168.188.240:3002/api/embed"
payload = {
    "model": "nomic-embed-text",
    "input": "Hi ",
}

response = requests.post(url, json=payload)

# Access the response data
data = response.json()
print(data)



#REST API
import requests
url = "http://192.168.188.240:3002/api/generate"
payload = {
    "model": "llama3.1",
    "prompt": "What LLM model are you",
    "stream": False
}

response = requests.post(url, json=payload)

# Access the response data
data = response.json()
print(data)


#LANGCHAIN
from langchain_ollama.llms import OllamaLLM

model = OllamaLLM(model="llama3.1", base_url="192.168.188.240:3002")
print(model.invoke("Can you tell me the top 5 tourist destinations in Singapore?"))




#LANGCHAIN
from langchain_core.prompts import ChatPromptTemplate
from langchain_ollama.llms import OllamaLLM

template = """Question: {question}

Answer: Let's think step by step."""

prompt = ChatPromptTemplate.from_template(template)

model = OllamaLLM(model="llama3.1", base_url="192.168.188.240:3002")

chain = prompt | model

print(chain.invoke({"question": "Can you tell me the top 5 tourist destinations in Singapore?"}))


#REST API
import requests, base64

with open("/data/data/testing_scripts/image1_1.jpeg", "rb") as f:
    img_b64 = base64.b64encode(f.read()).decode()

url = "http://192.168.188.240:3002/api/generate"
payload = {
    "model": "qwen2.5vl:7b", 
    "prompt": "Describe the content of this image", 
    "images": [img_b64],
    "stream": False
}

response = requests.post(url, json=payload)
print(response.json())
print(response.json()["response"])


#LangChain Wrapper
import base64
from langchain_core.messages import HumanMessage
from langchain_ollama import ChatOllama

with open("/data/data/testing_scripts/image1_1.jpeg", "rb") as f:
    img_b64 = base64.b64encode(f.read()).decode()

chat = ChatOllama(model="qwen2.5vl:7b", base_url="http://192.168.188.240:3002")

msg = HumanMessage(content=[
    {"type": "text", "text": "What is shown in this image?"},
    {"type": "image_url", "image_url": "data:image/jpeg;base64," + img_b64},
])

response = chat.invoke([msg])
print(response.content)


